{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntrajic/generative-nlp-with-variational-autoencoders-3834359/blob/main/Chapter1/Exercises/Chapter1Demo3_CustomLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers"
      ],
      "metadata": {
        "id": "qxIJkTKx-3qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Keras model with multiple outputs and a custom sampling layer, similar to those used in Variational AutoEncoders (VAEs), is an interesting task. Additionally, we'll incorporate custom loss functions for each output. Here's how we can achieve this:\n",
        "\n",
        "Here's how we can do it:\n",
        "\n",
        "1. Define the Custom Sampling Layer: This layer will be used to sample from a probability distribution, typically used in VAEs.\n",
        "\n",
        "1. Define the Model Architecture: The model will include the custom sampling layer and have two outputs.\n",
        "\n",
        "1. Implement Custom Loss Functions:\n",
        "        - Custom Binary Classification Loss: A simple example could be a variant of binary cross-entropy.\n",
        "        - KL Divergence Loss: TensorFlow provides a function for KL Divergence, which we could use directly but in this case we will provide a variation\n",
        "\n",
        "1. Generate Dummy Data: We'll create dummy data appropriate for our model's input and output specifications.\n",
        "\n",
        "1. Compile and Train the Model: We'll compile the model with our custom loss functions and then train it.\n",
        "\n"
      ],
      "metadata": {
        "id": "6iKEdMsL-69n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import KLDivergence\n",
        "import numpy as np\n",
        "\n",
        "# 1. Custom Sampling Layer\n",
        "\n",
        "class SamplingLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZJA14-H-6Yu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Model architecture\n",
        "input_layer = Input(shape=(10,))\n",
        "dense_layer = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# For the VAE-like output\n",
        "z_mean = Dense(10)(dense_layer)\n",
        "z_log_var = Dense(10)(dense_layer)\n",
        "sampling_output = SamplingLayer()([z_mean, z_log_var])\n",
        "\n",
        "output1 = Dense(1, activation='sigmoid', name='output1')(dense_layer)  # Binary classification output\n",
        "output2 = Dense(5, activation='softmax', name='output2')(sampling_output)  # Multiclass classification output using the sampled data\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=[output1, output2])"
      ],
      "metadata": {
        "id": "0uaLqKFxCtDH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Custom binary classification loss\n",
        "def custom_binary_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    epsilon = 1e-15\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "\n",
        "# 3. Custom KL Divergence Loss\n",
        "def custom_kl_divergence_loss(y_true, y_pred, scale_factor=1.0):\n",
        "    kl_loss = tf.keras.losses.KLDivergence()(y_true, y_pred)\n",
        "    return scale_factor * kl_loss\n"
      ],
      "metadata": {
        "id": "ird5Kzba_x_V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Generate dummy data\n",
        "x_dummy = np.random.random((1000, 10))\n",
        "y_dummy_output1 = np.random.randint(2, size=(1000, 1))\n",
        "y_dummy_output2 = np.random.randint(5, size=(1000, 5))\n",
        "\n"
      ],
      "metadata": {
        "id": "7c_hK1DK_01U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'output1': custom_binary_loss, 'output2': lambda y_true, y_pred: custom_kl_divergence_loss(y_true, y_pred, scale_factor=2.0)},\n",
        "              metrics={'output1': ['accuracy'], 'output2': ['accuracy']})\n",
        "\n",
        "# 5. Train the model\n",
        "model.fit(x_dummy, {'output1': y_dummy_output1, 'output2': y_dummy_output2}, epochs=10)"
      ],
      "metadata": {
        "id": "KJ_Kw0cN_3ir",
        "outputId": "8c7576d6-fcf2-4d1b-a0ec-2ab64eb245f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - loss: 16.6669 - output1_accuracy: 0.5023 - output1_loss: 0.7014 - output2_accuracy: 0.2090 - output2_loss: 15.9647\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 15.0429 - output1_accuracy: 0.4673 - output1_loss: 0.7010 - output2_accuracy: 0.2152 - output2_loss: 14.3411\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.7077 - output1_accuracy: 0.4815 - output1_loss: 0.6975 - output2_accuracy: 0.2246 - output2_loss: 14.0116\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.9192 - output1_accuracy: 0.4720 - output1_loss: 0.6995 - output2_accuracy: 0.1804 - output2_loss: 13.2159\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.8051 - output1_accuracy: 0.4562 - output1_loss: 0.6985 - output2_accuracy: 0.2160 - output2_loss: 13.1075\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6374 - output1_accuracy: 0.5082 - output1_loss: 0.6934 - output2_accuracy: 0.1836 - output2_loss: 12.9453\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.4813 - output1_accuracy: 0.4915 - output1_loss: 0.6947 - output2_accuracy: 0.2157 - output2_loss: 12.7880\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6089 - output1_accuracy: 0.5169 - output1_loss: 0.6933 - output2_accuracy: 0.2306 - output2_loss: 12.9171\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.5862 - output1_accuracy: 0.4728 - output1_loss: 0.6954 - output2_accuracy: 0.2080 - output2_loss: 12.8870\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.5107 - output1_accuracy: 0.5172 - output1_loss: 0.6938 - output2_accuracy: 0.2132 - output2_loss: 12.8155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e65e570a450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a Keras model is trained, you can evaluate its performance on a test dataset and use it to make predictions. Continuing from the previous example, I'll show you how to:\n",
        "\n",
        "1. Evaluate the Model: We'll evaluate the model on a separate set of dummy data to see how it performs.\n",
        "\n",
        "2. Use the Model for Prediction: We'll use the model to make predictions based on new input data."
      ],
      "metadata": {
        "id": "i_x1B6c3_8W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Evaluate the model\n",
        "\n",
        "# Generate some dummy test data\n",
        "x_dummy_test = np.random.random((200, 10))\n",
        "y_dummy_test_output1 = np.random.randint(2, size=(200, 1))  # Binary labels\n",
        "y_dummy_test_output2 = np.random.randint(5, size=(200, 5))  # One-hot encoded labels for 5 classes\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation = model.evaluate(x_dummy_test, {'output1': y_dummy_test_output1, 'output2': y_dummy_test_output2})\n",
        "print(f\"Test Loss, Test Accuracy for Output 1: {evaluation[1]}, {evaluation[3]}\")\n",
        "print(f\"Test Loss, Test Accuracy for Output 2: {evaluation[2]}, {evaluation[4]}\")\n"
      ],
      "metadata": {
        "id": "dNf8dv9ZAB56",
        "outputId": "59a0a0f2-6fa3-4c10-de56-f9097ec9ac15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - loss: 13.5006 - output1_accuracy: 0.4980 - output1_loss: 0.6923 - output2_accuracy: 0.2996 - output2_loss: 12.8210\n",
            "Test Loss, Test Accuracy for Output 1: 0.6931143403053284, 0.4749999940395355\n",
            "Test Loss, Test Accuracy for Output 2: 12.927233695983887, 0.2750000059604645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Use the model for prediction\n",
        "\n",
        "# New sample data for prediction\n",
        "new_sample = np.random.random((1, 10))\n",
        "\n",
        "# Making predictions\n",
        "predictions = model.predict(new_sample)\n",
        "print(f\"Predictions for Output 1 (Binary classification): {predictions[0]}\")\n",
        "print(f\"Predictions for Output 2 (Multiclass classification): {predictions[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "22brxmbSAFmf",
        "outputId": "4f5f114c-53e8-468e-d4ae-4998776a498c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531ms/step\n",
            "Predictions for Output 1 (Binary classification): [[0.4885618]]\n",
            "Predictions for Output 2 (Multiclass classification): [[0.22588544 0.19939019 0.20238237 0.18684217 0.18549985]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGxalfR4BAUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}